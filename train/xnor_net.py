""" Class and method definition for the layers in XNOR-Net
"""
import theano
import theano.tensor.nnet
import numpy as np
import lasagne
import theano.tensor as T
import time
from external.bnn_utils import binary_tanh_unit

def binarize_conv_filters(W):
    """Binarize convolution weights and find the weight scaling factor
    W : theano tensor : convolution layer weight of dimension no_filters x no_feat_maps x h x w
    """
    # symbolic binary weight
    Wb = T.cast(T.switch(T.ge(W, 0),1,-1), theano.config.floatX)  #Wb = sign(W)
      
    alpha = T.mean( T.reshape(T.abs_(W), (W.shape[0], W.shape[1]*W.shape[2]*W.shape[3])), axis=1) #每个卷积核单独计算

    return Wb, alpha

def binarize_conv_input(conv_input, k):

    bin_conv_out = binary_tanh_unit(conv_input)  # x[x<0] = -1   x[x>=0] = 1

    # scaling factor for the activation.
    A =T.abs_(conv_input)

    # K will have scaling matrixces for each input in the batch.
    
    k_shape = k.eval().shape  #（batch_size,channel,height,width)  k的height，width 和卷积核的height,width一致
    pad = (k_shape[-2]/2, k_shape[-1]/2)  #k的height和width都是奇数
    # support the kernel stride. This is necessary for AlexNet
    K = theano.tensor.nnet.conv2d(A, k, border_mode=pad)    # K's shape = (batch_size, 1, map_height, map_width)

    return bin_conv_out, K #输入的二值化
    


def binarize_fc_weights(W):
    # symbolic binary weight
    Wb = T.cast(T.switch(T.ge(W, 0),1,-1), theano.config.floatX)  # Wb = sign(W) 

    alpha = T.mean(T.abs_(W), axis=0)
    return Wb, alpha

def binarize_fc_input(fc_input):   ####这里怎么不变形状成为向量呢？？？？？？？？？？？？？？是因为有其他地方去处理吗

    bin_out = binary_tanh_unit(fc_input)
    
    if(fc_input.ndim == 4):  # prev layer is conv or pooling. hence compute the l1 norm using all maps
        beta = T.mean(T.abs_(fc_input), axis=[1, 2, 3])

    else: # feeding layer is FC layer
        beta = T.mean(T.abs_(fc_input), axis=1)

    return bin_out, beta



class Conv2DLayer(lasagne.layers.Conv2DLayer):
    """ Binary convolution layer which performs convolution using XNOR and popcount operations.
    This is followed by the scaling with input and weight scaling factors K and alpha respectively.
    """

    def __init__(self, incoming, num_filters, filter_size, xnor=True, **kwargs):

        """
        Parameters
        -----------
        incoming : layer or tuple
            Ipnut layer to this layer. If this is fed by a data layer then this is a tuple representing input dimensions.
        num_filters: int
            Number of 3D filters present in this layer = No of feature maps generated by this layer
        filter_size: tuple
            Filter size of this layer. Leading dimension is = no of input feature maps.
        """
        self.xnor = xnor    

        # average filter to compute scaling factor for activation
        no_inputs = incoming.output_shape[1]       #输入数据的channel
        shape = (num_filters, no_inputs, filter_size[0], filter_size[1])  #(num_of_filters,channel,height,width) 卷积核


        num_inputs = int(np.prod(filter_size)*incoming.output_shape[1])   #卷积核的元素个数
        num_units = int(np.prod(filter_size)*num_filters)  #？？？？？？？？？？
        self.W_LR_scale = np.float32(1./np.sqrt(1.5 / (num_inputs + num_units))) #？？？？？？？？为啥啊

        if(self.xnor):
            #xnor参数初始化保证在-1~1之间
            super(Conv2DLayer, self).__init__(incoming,num_filters, filter_size, W=lasagne.init.Uniform((-1, 1)), **kwargs)  
            self.params[self.W] = set(['xnor'])   #？？？？？？？？
        else:
            super(Conv2DLayer, self).__init__(incoming, num_filters, filter_size, **kwargs)


        if self.xnor:
            # beta用来近似输入
            #每个卷积核对应一个输入单元，这个输入单元要有一个beta_filter
            beta_filter = np.ones(shape=shape).astype(np.float32) / (no_inputs*filter_size[0]*filter_size[1])  #shape =  (num_of_filters,channel,height,width) 
            self.beta_filter = self.add_param(beta_filter, shape, name='beta_filter', trainable=False, regularizable=False)
            
            Wb = np.zeros(shape=self.W.shape.eval(), dtype=np.float32)  #存储二值化权重
            #xalpha用来近似权重
            xalpha = lasagne.init.Constant(0.1)   
            self.xalpha = self.add_param(xalpha, [num_filters,], name='xalpha', trainable=False, regularizable=False)
            #xalpha  对于每个filter计算一次 shape = (num_filters,)

    def convolve(self, input, deterministic=False, **kwargs): 
        """ Binary convolution. Both inputs and weights are binary (+1 or -1)
        This overrides convolve operation from Conv2DLayer implementation
        """
        if(self.xnor):
            # compute the binary inputs H and the scaling matrix K
            input, K = binarize_conv_input(input, self.beta_filter)  #二值化的输入和K

            # Compute the binarized filters are the scaling matrix
            self.Wb, alpha = binarize_conv_filters(self.W)  #二值化的权重和alpha
            
            if not deterministic:  #训练过程
                old_alpha = theano.clone(self.xalpha, share_inputs=False) #赋值
                old_alpha.default_update = alpha 
                alpha += 0*old_alpha
            else:#不训练
                alpha = self.xalpha 
            
            #alpha.shape = (num_of_filters,)
         
            Wr = self.W #Wr保持原来的初始的参数

            self.W = self.Wb  #网络使用二值化的参数去进行卷积forward，backward

            feat_maps = super(Conv2DLayer, self).convolve(input, **kwargs)  #二值化的输入和二值化的权重的卷积结果
            
            self.W = Wr #恢复原始精度

            #注意，这里有问题，Conv2D卷积操作自动添加了bias,应该最后再加的。。。。FIXME
            feat_maps = feat_maps * K #点乘，K需要扩维广播

            feat_maps = feat_maps * alpha.dimshuffle('x', 0, 'x', 'x')  #alpha 扩维成(1,num_of_filters,1,1)，再广播运算  #最终的近似卷积结果
            
        else:#普通卷积
            feat_maps = super(Conv2DLayer, self).convolve(input, **kwargs)
    
        return feat_maps

class DenseLayer(lasagne.layers.DenseLayer):
    """Binary version of fully connected layer. XNOR and bitcount ops are used for 
    this in a similar fashion as that of Conv Layer.
    """

    def __init__(self, incoming, num_units, xnor=True, **kwargs):
        """ XNOR-Net fully connected layer
        """
        self.xnor = xnor
        num_inputs = int(np.prod(incoming.output_shape[1:])) #输入的元素数（单个输入，no batch)
        self.W_LR_scale = np.float32(1./np.sqrt(1.5/ (num_inputs + num_units)))   #num_units是全连接层的输出单元数
        if(self.xnor):
            super(DenseLayer, self).__init__(incoming, num_units,  W=lasagne.init.Uniform((-1, 1)), **kwargs)
            self.params[self.W]=set(['xnor'])
        else:
            super(DenseLayer, self).__init__(incoming, num_units, **kwargs)

        if self.xnor:
            #Wb = np.zeros(shape=self.W.shape.eval(), dtype=np.float32)
            xalpha = np.zeros(shape=(num_units,), dtype=np.float32) #权重近似系数
            self.xalpha = self.add_param(xalpha, xalpha.shape, name='xalpha', trainable=False, regularizable=False)
            #self.Wb = self.add_param(Wb, Wb.shape, name='Wb', trainable=False, regularizable=False)

    def get_output_for(self, input, deterministic=False, **kwargs):
        """ Binary dense layer dot product computation
        """
        if(self.xnor):
            # binarize the input
            bin_input, beta = binarize_fc_input(input)

            # compute weight scaling factor.
            self.Wb, alpha = binarize_fc_weights(self.W)
            
            if not deterministic:#训练
                old_alpha = theano.clone(self.xalpha, share_inputs=False)
                old_alpha.default_update = alpha  #？？？？？？？？？？？？？？
                alpha += 0*old_alpha
            else:#不训练
                alpha = self.xalpha

            #W_full_precision = self.Wb * alpha.dimshuffle('x', 0)
            Wr = self.W
            self.W = self.Wb
            
            #同样的，这里有问题，不应该先添加bias
            fc_out = super(DenseLayer, self).get_output_for(bin_input, **kwargs)
            
            fc_out = fc_out * beta.dimshuffle(0, 'x')

            fc_out = fc_out * alpha.dimshuffle('x', 0)
            
            #self.W = W_full_precision
            self.W = Wr
        else:
            fc_out = super(DenseLayer, self).get_output_for(input, **kwargs)

        return fc_out

        # find the dot product
        # scale the output by alpha and beta

